{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "--------------\n",
    "DataLoader is a generic class that loads data from csv files in a directory into a database or HDF file. It has two requirements\n",
    " 1. All files in the directory should be of the same structure. Structure means all files must have the same columns and the same datatype\n",
    " 2. There should be no sub directory inside the directory\n",
    " \n",
    "*The examples in this notebook don't run automatically. You need to create your own directory and tables before getting started and then substitute the variables*\n",
    " \n",
    "To get your data into a database you need a \n",
    " 1. sqlalchemy connection string\n",
    " 2. tablename in which data is to be loaded\n",
    "\n",
    "Let's start with a simple example by loading our files into a in-memory SQLITE database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from fastbt.loaders import DataLoader\n",
    "engine = create_engine('sqlite://')\n",
    "directory = 'data'\n",
    "tablename = 'table'\n",
    "\n",
    "dl = DataLoader(directory=directory, mode='SQL', engine=engine,\n",
    "               tablename=tablename)\n",
    "dl.load_data()\n",
    "\n",
    "# If you have trouble with your directory names, use absolute file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also load your data into a HDF5 file. Just specify mode as HDF and engine as the HDF5 filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = 'data.h5'\n",
    "directory = 'data'\n",
    "tablename = 'table'\n",
    "\n",
    "dl = DataLoader(directory=directory, mode='HDF', engine=engine,\n",
    "               tablename=tablename)\n",
    "dl.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have new files in the directory, run ``dl.load_data()`` again.\n",
    "Your database would be updated with new data.\n",
    "\n",
    "Internally, each of your file is also stored in the database as a separate table; so if you make any changes to your directory you can run the ``load_data()`` function to update your database.\n",
    "Information about files is stored in a separate table beginning with *updated_* in SQL and under the */updated* hirerachy in HDF\n",
    "\n",
    "**Don't name your tables starting with updated. Use a different name** \n",
    "\n",
    "This would clash with the internal naming of the files while loading into database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could load your data into any database by passing the appropriate sqlalchemy connection string. See the official sqlalchemy page  for more [details](http://docs.sqlalchemy.org/en/latest/core/engines.html).\n",
    "\n",
    "Let's try loading our data into a MYSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('mysql://scott:tiger@localhost/foo')\n",
    "directory = 'data'\n",
    "tablename = 'table'\n",
    "\n",
    "dl = DataLoader(directory=directory, mode='SQL', engine=engine,\n",
    "               tablename=tablename)\n",
    "dl.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``load_data`` function actually loads the data into your database and it accepts other arguments. Infact, its just a wrapper to the pandas ``read_csv`` function and you can pass any of the arguments of the ``read_csv`` function to this function as keyword arguments.\n",
    "\n",
    "Once you specify arguments to the load_data function, you must use the same arguments each time. If you need any changes midway, drop the earlier table and create a new table with a different name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can rename columns in your files in the database with the ``rename`` argument . This is pretty useful when your files has headers with spaces.\n",
    "\n",
    "Say you want to rename the columns **Daily Volume and timestamp** in your files to **volume and date** in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with columns in file as keys and\n",
    "# the columns required in database as values\n",
    "rename = {\n",
    "    'Daily Volume' : 'volume',\n",
    "    'timestamp': 'date'    \n",
    "}\n",
    "dl.load_data(rename=rename)\n",
    "\n",
    "# This would load data into database with volume and date as columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use ``parse_dates`` to parse columns as datetime.\n",
    "\n",
    "Parse the columns date and timestamp in the files to datetime type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column to datetime type\n",
    "dl.load_data(parse_dates=['date', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also specify custom datetime format\n",
    "dl.load_data(parse_dates=['date', 'timestamp'], datetime_format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use any of the pandas arguments to the read_csv function. So if you need to load only columns 2,3,4 and skip the first 10 rows then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.load_data(usecols=[1,2,3], skiprows=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though only csv files are preferred, you can also load tab delimited \n",
    "text files if it corresponds to csv format. So if you have a file with\n",
    "first 6 rows as metadata and seventh row as headers with the delimiter\n",
    "being a vertical dash **|**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.load_data(skiprows=6, delimiter='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to run any other function after the file is read, then you could use the ``postfunc`` argument.\n",
    "\n",
    "postfunc should be a function with three mandatory arguments \n",
    " 1. first argument is the dataframe from reading the file\n",
    " 2. second argument is the filename of the file being read\n",
    " 3. third argument is the directory\n",
    "The function must return a dataframe\n",
    " \n",
    "The three arguments are automatically supplied to the function when\n",
    "loading into the database. You need to write only the logic based on\n",
    "the above three function. Pass the function as an argument.\n",
    " \n",
    "Let's add a function that would add the filename to our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postfunc(df, filename, root):\n",
    "    df['filename'] = filename\n",
    "    return df\n",
    "\n",
    "dl.load_data(postfunc=postfunc)\n",
    "\n",
    "# Now the column filename would be included in the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cases for ``postfunc`` function\n",
    "\n",
    " * Add some file/ directory specific data\n",
    " * Transform data\n",
    " * Add a few columns\n",
    " * Filter data based on some condition \n",
    " * and any other use case you may think of\n",
    " \n",
    "Let's see a example to calculate the range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postfunc(df, filename, root):\n",
    "    df['range'] = df['high'] - df['low']\n",
    "    return df\n",
    "\n",
    "dl.load_data(postfunc=postfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get datetime from filename\n",
    "# Assume your filename is in the format 2018-01-01\n",
    "import datetime\n",
    "def postfunc(df, filename, root):\n",
    "    df['date'] = datetime.datetime.strptime(filename, '%Y-%m-%d')    \n",
    "\n",
    "dl.load_data(postfunc=postfunc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just need to load data only without writing to a database or need a\n",
    "more flexible function to read your files, use the ``collate_data`` function. The first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads all the files into a dataframe\n",
    "from fastbt.loaders import collate_data\n",
    "directory = 'data'\n",
    "df = collate_data(directory=directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few more examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'data'\n",
    "\n",
    "# Parse dates\n",
    "df = collate_data(directory=directory, parse_dates=['date', 'timestamp'])\n",
    "\n",
    "# Use skiprows\n",
    "df = collate_data(directory=directory, parse_dates=['date', 'timestamp'],\n",
    "                 skiprows=10)\n",
    "\n",
    "# Use a custom delimiter\n",
    "df = collate_data(directory=directory, parse_dates=['date', 'timestamp'],\n",
    "                 skiprows=10, delimiter='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need an entirely customized function, then create a function with a\n",
    "single argument. The argument is just the filename and it must return a dataframe. This is useful when you are dealing with different\n",
    "file formats.\n",
    "\n",
    "Let's load a list of json files into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'data'\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def function(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        result = json.load(f)    \n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "df = collate_data(directory=directory, function=function)\n",
    "\n",
    "# Note kwargs won't be passed if you are using a custom function        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
