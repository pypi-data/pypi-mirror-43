# coding: utf8
"""
A tool for parsing Scrapy logfiles periodically and incrementally, designed for ScrapydWeb.

GitHub: https://github.com/my8100/logparser
"""


###############################################################################
###############################################################################
## Make sure that [Scrapyd](https://github.com/scrapy/scrapyd) has been installed
## and started on the current host.
## ------------------------------ Chinese -------------------------------------
## 请先确保当前主机已经安装和启动 [Scrapyd](https://github.com/scrapy/scrapyd)。
###############################################################################
###############################################################################


############################## Basic Settings ##############################
# Only use for 'json_url' in the generated file 'stats.json' resides in SCRAPYD_LOGS_DIR.
# The default is '127.0.0.1:6800', so that the stats of Scrapyd jobs can be accessed at:
# http://127.0.0.1:6800/logs/stats.json
SCRAPYD_SERVER = '127.0.0.1:6800'

# The directory in which LogParser would locate and parse the Scrapy logfiles.
# Check out below url to find out where the Scrapy logs are stored:
# https://scrapyd.readthedocs.io/en/stable/config.html#logs-dir
# e.g. 'C:/Users/username/logs/' or '/home/username/logs/'
SCRAPYD_LOGS_DIR = ''

# Sleep N seconds before starting next round of parsing, the default is 60.
PARSE_ROUND_INTERVAL = 60


############################## Advanced Settings ##############################
# Whether to collect crawler.stats and engine status via telnet, the default is True.
# Check out https://doc.scrapy.org/en/latest/topics/telnetconsole.html for more info.
# Note that this feature temporarily only works for Scrapy 1.5.1 and its earlier version,
# since telnet console now requires username and password after Scrapy 1.5.2,
# and this option would be set to False if unsupported version of Scrapy is found.
ENABLE_TELNET = True

# The default is '', set up this option only when you are using docker-compose.
# e.g. '127.0.0.1'
OVERRIDE_TELNET_CONSOLE_HOST = ''

# The encoding of the Scrapy logs, the default is 'utf-8'.
# https://doc.scrapy.org/en/latest/topics/settings.html#log-encoding
LOG_ENCODING = 'utf-8'

# LogParser would locate and parse the Scrapy logfiles with a specific extension.
# The default is ['.log', '.txt'].
LOG_EXTENSIONS = ['.log', '.txt']

# Extract the first N lines of the Scrapy log, the default is 100.
LOG_HEAD_LINES = 100

# Extract the last N lines of the Scrapy log, the default is 200.
LOG_TAIL_LINES = 200

# Whether to delete existing json files generated by LogParser at startup, the default is False.
DELETE_EXISTING_JSON_FILES_AT_STARTUP = False

# Whether to keep all parsed results in memory, the default is False.
# Set it to False to reduce the RAM usage of LogParser.
KEEP_DATA_IN_MEMORY = False

# Set up this option to limit the size of the generated file 'stats.json' resides in SCRAPYD_LOGS_DIR,
# by removing data of the deleted Scrapy logs, the default is 100.
JOBS_TO_KEEP = 100

# Limit the memory usage when parsing large logfiles.
# The default is 10 * 1000 * 1000, equals to 10 MB.
CHUNK_SIZE = 10 * 1000 * 1000

# The default is False, set it to True to set the logging level from WARNING to DEBUG
VERBOSE = False
