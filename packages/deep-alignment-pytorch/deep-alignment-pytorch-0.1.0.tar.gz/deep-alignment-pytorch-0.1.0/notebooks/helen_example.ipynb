{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Alignement Network with [delira](https://github.com/justusschock/delira)\n",
    "\n",
    "The following example shows the basic usage of the provided DAN implementation with `delira`\n",
    "\n",
    "First we need to download our data. For training, we use the HELEN Dataset, which can be downloaded [here](https://ibug.doc.ic.ac.uk/download/annotations/helen.zip).\n",
    "\n",
    "> **Note**: Since this dataset contains only a small amount of data, you may want to download the other datasets on this website as well and add them to your trainset.\n",
    "\n",
    "To automate the necessary preprocessing, please insert the path to the downloaded zip-file below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = \"/PATH/TO/YOUR/ZIPFILE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to preprocess our dataset, which is simply extracting it and calculating the mean face. TO extract it, we use the libraries `zipfile` and `os`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# create directory \"dataset\" in same directory as zip file\n",
    "image_dir = os.path.join(os.path.split(zip_path)[0],\"dataset\")\n",
    "\n",
    "# if there is not an dataset already\n",
    "if not os.path.isdir(image_dir):\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path) as zip_ref:\n",
    "            zip_ref.extractall(image_dir)\n",
    "        \n",
    "train_path = os.path.join(image_dir, \"trainset\")\n",
    "test_path = os.path.join(image_dir, \"testset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to calculate the trainset's mean shape. For this, we use `numpy` and `shapedata`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shapedata\n",
    "\n",
    "# if mean shape has not been computed yet:\n",
    "if not os.path.isfile(os.path.join(image_dir, \"mean_shape.npz\")):\n",
    "\n",
    "    # Loading whole traindata\n",
    "    data = shapedata.SingleShapeDataProcessing.from_dir(train_path)\n",
    "\n",
    "    # store landmarks as numpy array and calculate mean\n",
    "    landmarks = np.array(data.landmarks)\n",
    "    mean_shape = landmarks.mean(axis=0)\n",
    "\n",
    "    # save mean_shape to disk\n",
    "    np.savez_compressed(os.path.join(image_dir, \"mean_shape.npz\"), mean_shape=mean_shape)\n",
    "    \n",
    "else:\n",
    "    mean_shape = np.load(os.path.join(image_dir, \"mean_shape.npz\"))[\"mean_shape\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just have to do these steps once, now you can simply load the mean_shape with `np.load(os.path.join(image_dir, \"mean_shape.npz\"))[\"mean_shape\"]`\n",
    "\n",
    "Now we will create our datasets (with classes from `shapedata` and `delira`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delira.data_loading import BaseDataManager\n",
    "from delira.data_loading.sampler import RandomSampler, SequentialSampler\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# some augmentations for train data:\n",
    "IMG_SIZE = 112\n",
    "CROP = 1.\n",
    "EXTENSION = \".pts\"\n",
    "ROTATE = 90\n",
    "RANDOM_OFFSET = 50\n",
    "RANDOM_SCALE = 0.25\n",
    "\n",
    "# create trainset with augmentations\n",
    "dset_train = shapedata.SingleShapeDataset(train_path,\n",
    "                                         img_size=IMG_SIZE,\n",
    "                                         crop=CROP,\n",
    "                                         extension=EXTENSION,\n",
    "                                         rotate=ROTATE,\n",
    "                                         random_offset=RANDOM_OFFSET,\n",
    "                                         random_scale=RANDOM_SCALE\n",
    "                                         )\n",
    "\n",
    "# create testset without augmentations\n",
    "dset_test = shapedata.SingleShapeDataset(test_path,\n",
    "                                         img_size=IMG_SIZE,\n",
    "                                         crop=CROP,\n",
    "                                         extension=EXTENSION,\n",
    "                                         rotate=None,\n",
    "                                         random_offset=False,\n",
    "                                         random_scale=False\n",
    "                                         )\n",
    "\n",
    "# create data managers out of datasets\n",
    "man_train = BaseDataManager(dset_train, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            n_process_augmentation=4, \n",
    "                            transforms=None, \n",
    "                            sampler_cls=RandomSampler)\n",
    "man_test = BaseDataManager(dset_test, \n",
    "                           batch_size=BATCH_SIZE, \n",
    "                           n_process_augmentation=4, \n",
    "                           transforms=None, \n",
    "                           sampler_cls=SequentialSampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have defined our datasets for images of size 224x224 pixels, we need to take care of our model definition.\n",
    "Now we need to define our training and model arguments using the `Parameters` class from `delira` and some functions and classes given in this package (here we import it for the first time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dan\n",
    "\n",
    "from delira.training import Parameters\n",
    "import torch\n",
    "\n",
    "callback_stages = dan.AddDanStagesCallback(epoch_freq=50)\n",
    "\n",
    "params = Parameters(\n",
    "    fixed_params={\n",
    "        \"training\":{\n",
    "            \"num_epochs\": 100,\n",
    "            \"criterions\": {\n",
    "                \"points\": torch.nn.L1Loss()\n",
    "            },\n",
    "            \"optimizer_cls\": torch.optim.Adam,\n",
    "            \"optimizer_params\":{\n",
    "                \"max_stages\": 2\n",
    "            }, \n",
    "            \"metrics\": {\"MSE\": torch.nn.MSELoss()},\n",
    "            \"callbacks\": [callback_stages],\n",
    "            \"lr_sched_cls\": None,\n",
    "            \"lr_sched_params\": {}\n",
    "        }, \n",
    "        \"model\":\n",
    "        {\n",
    "            \"mean_shape\": mean_shape,\n",
    "            \"num_stages\": 2,\n",
    "            \"return_intermediate_lmks\": True\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally! Now, we can start our training using the `PyTorchExperiment`.\n",
    "\n",
    "We just do a few minor specifications here:\n",
    "\n",
    "* set the usable GPUs to the first available GPU if any GPUs have been detected (else specify the usable GPUs to be empty, which causes a training on CPU)\n",
    "* use the `create_optimizers_dan_per_stage` to automatically create optimizers for our DeepAlinmentNetwork (you could also use `create_optimizers_dan_whole_network` to create a single optimizer holding all network parameters)\n",
    "* use the `DeepAlignmentNetwork` class as our network, which defines the training and prediction behavior.\n",
    "\n",
    "Now let's start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delira.training import PyTorchExperiment\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_ids = [0]\n",
    "else:\n",
    "    gpu_ids = []\n",
    "\n",
    "exp = PyTorchExperiment(params, \n",
    "                        dan.DeepAlignmentNetwork, \n",
    "                        optim_builder=dan.create_optimizers_dan_per_stage, \n",
    "                        gpu_ids=gpu_ids,\n",
    "                        val_score_key=\"val_MSE_final_stage\")\n",
    "exp.run(man_train, man_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
